---
title: "Brian May Instagram Analytics"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages('tidyverse')
#install.packages('dplyr')
```

```{r}
library(stringr)
setwd("/Users/irisgu/Desktop/me/Music & Fun/Brianinstagram")
instaposts=read.csv("brianins.csv")
instaposts=instaposts[,-3]
```

#Change variable type from factors into character
```{r}
instaposts$ownerUsername=as.character(instaposts$ownerUsername)
instaposts$firstComment=as.character(instaposts$firstComment)
instaposts$alt=as.character(instaposts$alt)
instaposts$locationName=as.character(instaposts$locationName)
instaposts$url=as.character(instaposts$url)
instaposts$timestamp=as.character(instaposts$timestamp)
str(instaposts)
```
#Leave only BrianMay's own posts
```{r}
instaposts=instaposts[instaposts$ownerUsername=='brianmayforreal',]
```
#change column name
```{r}
colnames(instaposts)[colnames(instaposts)=="firstComment"] = "text"
str(instaposts)
```

#### post length
post Length in characters
```{r, warning=F,message=F}
instaposts$char_count=nchar(instaposts$text)
```

```{r}
library(dplyr)
library(stats)
library(base)
instaposts=distinct(instaposts)
instaposts$id <- seq.int(nrow(instaposts))
mean(instaposts$char_count)
```

post Length in words  
```{r, warning=F,message=F}
library(stringr)
instaposts$word_count=str_count(string = instaposts$text,pattern = '\\S+')
max(instaposts$word_count)
instaposts[instaposts$word_count==411,]$text
```

post Length in sentences  
```{r, warning=F,message=F}
instaposts$sentence_count=str_count(string = instaposts$text,pattern = "[A-Za-z,;'\"\\s]+[^.!?]*[.?!]")
max(instaposts$sentence_count)
instaposts[instaposts$sentence_count==34,]$text
```

post length in characters
```{r}
instaposts$char_count=nchar(instaposts$text)
mean(instaposts$char_count)
max(instaposts$char_count)
instaposts[instaposts$char_count==2199,]$text
```

```{r}
instaposts=instaposts[instaposts$word_count!=0,]
instaposts=instaposts[instaposts$sentence_count!=0,]
instaposts=instaposts[instaposts$char_count!=0,]
instaposts$text[max(instaposts$char_count)]
instaposts$text[max(instaposts$word_count)]
instaposts$text[max(instaposts$sentence_count)]
```

#### Number of characters in post NO.xxx
```{r, warning=F,message=F}
instaposts$text[617]
nchar(instaposts$text[617])
```
#### Screaming posts 
```{r}
proportionUpper = str_count(instaposts$text,pattern='[A-Z]')/instaposts$char_count
#cor(proportionUpper,instaposts$text)
#cor.test(proportionUpper,instaposts$text)
mean(proportionUpper)
```

#### Exclamation marks 
```{r}
summary(str_count(instaposts$text,pattern='!')) 
long_exclamation_post_index=which.max(str_count(instaposts$text,pattern='!'))
instaposts$text[long_exclamation_post_index]
```

#### Most Common Words
Here we will list the top 25 frequently occuring words. 
```{r}
library(qdap)
freq_terms(text.var = instaposts$text,top = 25)
plot(freq_terms(text.var = instaposts$text,top = 25))
```


```{r, warning=F,message=F}
stopwords = c(Top200Words,"bri","http","drbrianmay","im","it's","rt","httpstcouyachztqal","httpstcor","httpstco","i'm",'d')
freq_terms(text.var=instaposts$text,top=25,stopwords = stopwords)
plot(freq_terms(text.var=instaposts$text,top=25,stopwords = stopwords))
```

#### Words that make a post likable
Which words are tied to posts that are liked more?  
First drop top200 stopwords and some other words that occur most but have little diagnostic value. 
```{r, warning=F,message=F}
words_by_rating = word_list(instaposts$text,grouping.var=tolower(instaposts$text),
                stopwords =c(Top200Words,"bri","http","drbrianmay","it's","rt","httpstcouyachztqal","httpstcor","httpstco","i'm",'d',"im"),
                            cut.n = 25)
```

#### Words in Posts
Let us begin by counting the number of words in each post.  
```{r, warning=F,message=F}
library(dplyr)
library(tidytext)
word=instaposts%>%
  select(id,text)%>%
  group_by(id)%>%
  unnest_tokens(output = word,input=text)%>%
  count()

```

#### Total words
So, what is the total number of words in all posts?  
```{r, warning=F,message=F}
instaposts %>%
  select(id,text)%>%
  group_by(id)%>%
  unnest_tokens(output = word,input=text)%>%
  ungroup()%>%
  count()
```

#### Word Lexicons  

#### Bing Lexicon
 
The bing lexicon categorizes words as being positive and negative. 
Here are the first fifty words.   
```{r, warning=F,message=F}
as.data.frame(get_sentiments('bing'))[1:50,]
get_sentiments('bing')%>%
  group_by(sentiment)%>%
  count()
```

Now, let us explore valence of the words used in posts using the bing dictionary. We will match the words in the dictionary with the ones in the posts to determine valence.
```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)
```

#### Positive and Negative Words in instaposts
```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  count()
```

```{r, warning=F,message=F}
library(ggplot2); library(ggthemes)
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('bing'))%>%
  group_by(sentiment)%>%
  count()%>%
  ggplot(aes(x=sentiment,y=n,fill=sentiment))+geom_col()+theme_economist()+guides(fill=F)
```


#### nrc lexicon
A word may reflect more than just valence. The 'nrc' lexicon categorizes words by emotion.   
Here is a list of emotions covered by this lexicon.
```{r, warning=F,message=F}
library(remotes)
install_github("EmilHvitfeldt/textdata")
install_github("juliasilge/tidytext")
library(tidytext)
get_sentiments("nrc")

#install.packages('textdata')
#library(textdata)
```


```{r}
get_sentiments('nrc')
 # group_by(sentiment)%>%
#  count()
```


```{r, warning=F,message=F}
table(get_sentiments('nrc')$sentiment)  
```

#### Emotions in posts  
Let us examine the emotions expressed in the posts
```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('nrc'))%>%
  group_by(sentiment)%>%
  count()
```


```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('nrc'))%>%
  group_by(sentiment)%>%
  count()%>%
  ggplot(aes(x=reorder(sentiment,X = n),y=n,fill=sentiment))+geom_col()+guides(fill=F)+coord_flip()+theme_wsj()
```


#### LikesCount of each post based on Emotions Expressed
```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('nrc'))%>%
  group_by(id,sentiment,char_count)%>%
  count()
```

#### LikesCount of all posts based on Emotion Expressed  

#### Correlation between emotion expressed and post likesCount  
Relationship by examining the correlation.

```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('nrc'))%>%
  group_by(id,sentiment,likesCount)%>%
  count()%>%
  ungroup()%>%
  group_by(sentiment)%>%
  summarize(correlation = cor(n,likesCount))
```

#### Scatterplot of relationship
Let us examine scatterplots to gain a better understanding of the correlations. 
```{r, warning=F,message=F}
instaposts%>%
  group_by(id)%>%
  unnest_tokens(output = word, input = text)%>%
  inner_join(get_sentiments('nrc'))%>%
  group_by(id,sentiment,likesCount)%>%
  count()%>%
  ungroup()%>%
  group_by(sentiment)%>%
  ggplot(aes(x=likesCount,y=n))+geom_point()+facet_wrap(~sentiment)+geom_smooth(method='lm',se=F)
```


###Convert and clean document for text mining
```{r}
# Build a corpus
library(tm)
library(dplyr)
library(xtable)
docs <- Corpus(VectorSource(instaposts$text)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)

docs <- tm_map(docs, content_transformer(tolower))      # to lowercase
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
#Create own custom stopwords
my_custom_stopwords = c(Top200Words,"bri","http","drbrianmay","it's","rt","httpstcouyachztqal","httpstcor","'m","'s",
                        "'re","\U0001f4a5\U0001f4a5\U0001f4a5\U0001f4a5","'ve","â€”","'ll")
docs <- tm_map(docs, removeWords, my_custom_stopwords)

```

```{r}
#Term Frequency matrix
tdm <- TermDocumentMatrix(docs)
tdm.matrix <- as.matrix(tdm)
tdm.rs <- sort(rowSums(tdm.matrix), decreasing=TRUE)
tdm.df <- data.frame(word = names(tdm.rs), freq = tdm.rs, stringsAsFactors = FALSE)
```


```{r}
# Wordcloud
library(wordcloud)

set.seed(1234)
wordcloud(words = tdm.df$word, freq = tdm.df$freq, min.freq = 50,
          max.words=300, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```


#### Comparison Cloud
Finally, here is a comparison cloud to contrast positive and negative words in the posts. 
```{r, warning=F,message=F}
library(tidyr)
wordcloudData = 
  instaposts%>%
  group_by(id)%>%
  unnest_tokens(output=word,input=text)%>%
  anti_join(stop_words)%>%
  inner_join(get_sentiments('bing'))%>%
  ungroup()%>%
  count(sentiment,word,sort=T)%>%
  spread(key=sentiment,value = n,fill=0)%>%
  data.frame()
rownames(wordcloudData) = wordcloudData[,'word']
wordcloudData = wordcloudData[,c('positive','negative')]
set.seed(617)
comparison.cloud(term.matrix = wordcloudData,scale = c(2,0.5),max.words = 100, rot.per=0)
```

